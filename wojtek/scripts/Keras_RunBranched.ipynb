{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "import keras\n",
    "import keras_models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.optimizers import SGD, Adadelta, Adam, Nadam, RMSprop\n",
    "from keras.preprocessing import sequence, text\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: Conv1DLSTMbranchedV2Bidirectional_FastText_1bag_BS256_Nadam_SpacyClean\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "n_bags = 1\n",
    "split_size = 0.1\n",
    "max_features = 200000\n",
    "sequence_length = 196\n",
    "embedding_dim = 300\n",
    "create_embedding = False\n",
    "bidirectional = True\n",
    "\n",
    "run_prefix = 'FastText_'\n",
    "src = '/home/w/Projects/Toxic/data/'\n",
    "model_name = 'Conv1DLSTMbranchedV2'\n",
    "optimizer = 'Nadam'\n",
    "data_type = 'SpacyClean'\n",
    "kfold_run = 0\n",
    "batch_size = 256\n",
    "importance = 0\n",
    "stratify = 0\n",
    "save_models = 0\n",
    "load_models = 0\n",
    "save_oof = 0\n",
    "prepare_submission = 1\n",
    "\n",
    "\n",
    "if bidirectional and 'LSTM' in model_name or bidirectional and 'GRU' in model_name:\n",
    "    run_prefix = 'Bidirectional_{}'.format(run_prefix)\n",
    "if kfold_run:\n",
    "    general_run_name = '{}{}fold_BS{}_{}'.format(\n",
    "        run_prefix, n_folds, batch_size, optimizer)\n",
    "else:\n",
    "    general_run_name = '{}{}bag_BS{}_{}'.format(\n",
    "        run_prefix, n_bags, batch_size, optimizer)\n",
    "\n",
    "\n",
    "if len(data_type) > 0:\n",
    "    general_run_name += '_{}'.format(data_type)\n",
    "if importance:\n",
    "    general_run_name += '_ImportanceTrain'\n",
    "if stratify and kfold_run:\n",
    "    general_run_name += '_Stratified'\n",
    "\n",
    "run_name = '{}{}'.format(model_name, general_run_name)\n",
    "print('Run name: {}'.format(run_name))\n",
    "\n",
    "\n",
    "model_callbacks = [EarlyStopping(monitor='val_loss', patience=15, verbose=1),\n",
    "                   ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1,\n",
    "                                     patience=7, min_lr=1e-5)]\n",
    "\n",
    "if optimizer == 'Adam':\n",
    "    optimizer = Adam(lr=1e-3, decay=1e-3)\n",
    "    # optimizer = 'adam'\n",
    "if optimizer == 'Nadam':\n",
    "    optimizer = Nadam(lr=1e-3, schedule_decay=1e-3)\n",
    "    # optimizer = 'nadam'\n",
    "if optimizer == 'SGD':\n",
    "    optimizer = SGD(lr=1e-2, momentum=0.9,\n",
    "                    decay=1e-4, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_data = '/home/w/Projects/Toxic/data/features/'\n",
    "\n",
    "\n",
    "train = pd.read_pickle(\"../data/train_spacy_clean.pkl\")\n",
    "test = pd.read_pickle(\"../data/test_spacy_clean.pkl\")\n",
    "target_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "data_tokenized = pd.read_pickle(src_data + 'data_TokenizedSentences196len.pkl')\n",
    "data_badwords300 = pd.read_pickle(src_data + 'data_Binary300Badwords.pkl')\n",
    "data_badwordsCount = pd.read_pickle(src_data + 'data_BadwordsCount.pkl')\n",
    "\n",
    "X = pd.concat([data_tokenized, data_badwords300], axis=1)\n",
    "X['badwordsCount'] = data_badwordsCount\n",
    "\n",
    "X_train_mlp = X.iloc[:train.shape[0], :]\n",
    "X_test_mlp = X.iloc[train.shape[0]:, :]\n",
    "\n",
    "features = np.setdiff1d(X_train_mlp.columns, target_columns)\n",
    "\n",
    "del X, test\n",
    "del data_tokenized, data_badwords300, data_badwordsCount\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data cleaned with Spacy.\n",
      "(95851, 8) (226998, 2)\n",
      "(95851, 196) (95851, 6) (226998, 196)\n"
     ]
    }
   ],
   "source": [
    "train, test = utils.load_data(src, mode=data_type)\n",
    "print(train.shape, test.shape)\n",
    "list_classes = [\"toxic\", \"severe_toxic\",\n",
    "                \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index)) + 1\n",
    "\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=sequence_length)  # [:1000]\n",
    "y_train = train[list_classes].values  # [:1000]\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=sequence_length)  # [:1000]\n",
    "print(X_train.shape, y_train.shape, X_test.shape)\n",
    "\n",
    "del train, test, list_tokenized_train, list_tokenized_test\n",
    "gc.collect()\n",
    "\n",
    "if create_embedding:\n",
    "    embedding_file = '/home/w/Projects/Toxic/data/embeddings/GoogleNews-vectors-negative300.bin.gz'\n",
    "    word2vec = KeyedVectors.load_word2vec_format(embedding_file, binary=True)\n",
    "    print('Found %s word vectors of word2vec' % len(word2vec.vocab))\n",
    "\n",
    "    embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if word in word2vec.vocab:\n",
    "            embedding_matrix[i] = word2vec.word_vec(word)\n",
    "    print('Null word embeddings: %d' %\n",
    "          np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "else:\n",
    "    embedding_matrix = pd.read_pickle(\n",
    "        '../data/embeddings/FastText_300dim_embedding.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr, X_val, X_tr_mlp, X_val_mlp, y_tr, y_val =  train_test_split(X_train, X_train_mlp, y_train, test_size=0.1,\n",
    "                                                                  random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running parametrized bagging\n",
      "Running: Conv1DLSTMbranchedV2Bidirectional_FastText_1bag_BS256_Nadam_SpacyClean\n",
      "Training on bag: 1 \n",
      "\n",
      "Saving CSV logs for model from current bag/fold: Conv1DLSTMbranchedV2Bidirectional_FastText_1bag_BS256_Nadam_SpacyClean, bag number 1 \n",
      "\n",
      "Validating on subset of data specified by user.\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/1000\n",
      "86265/86265 [==============================] - 71s 827us/step - loss: 0.2077 - acc: 0.9359 - val_loss: 0.1038 - val_acc: 0.9732\n",
      "Epoch 2/1000\n",
      "86265/86265 [==============================] - 70s 814us/step - loss: 0.0634 - acc: 0.9807 - val_loss: 0.0556 - val_acc: 0.9813\n",
      "Epoch 3/1000\n",
      "86265/86265 [==============================] - 68s 793us/step - loss: 0.0510 - acc: 0.9822 - val_loss: 0.0522 - val_acc: 0.9812\n",
      "Epoch 4/1000\n",
      "86265/86265 [==============================] - 71s 824us/step - loss: 0.0451 - acc: 0.9833 - val_loss: 0.0548 - val_acc: 0.9812\n",
      "Epoch 5/1000\n",
      "86265/86265 [==============================] - 67s 775us/step - loss: 0.0421 - acc: 0.9842 - val_loss: 0.0484 - val_acc: 0.9824\n",
      "Epoch 6/1000\n",
      "86265/86265 [==============================] - 65s 757us/step - loss: 0.0386 - acc: 0.9853 - val_loss: 0.0469 - val_acc: 0.9827\n",
      "Epoch 7/1000\n",
      "86265/86265 [==============================] - 66s 770us/step - loss: 0.0359 - acc: 0.9860 - val_loss: 0.0490 - val_acc: 0.9823\n",
      "Epoch 8/1000\n",
      "86265/86265 [==============================] - 66s 761us/step - loss: 0.0338 - acc: 0.9866 - val_loss: 0.0499 - val_acc: 0.9818\n",
      "Epoch 9/1000\n",
      "86265/86265 [==============================] - 66s 769us/step - loss: 0.0312 - acc: 0.9875 - val_loss: 0.0556 - val_acc: 0.9825\n",
      "Epoch 10/1000\n",
      "86265/86265 [==============================] - 68s 787us/step - loss: 0.0296 - acc: 0.9883 - val_loss: 0.0541 - val_acc: 0.9831\n",
      "Epoch 11/1000\n",
      "86265/86265 [==============================] - 65s 758us/step - loss: 0.0278 - acc: 0.9888 - val_loss: 0.0537 - val_acc: 0.9824\n",
      "Epoch 12/1000\n",
      "86265/86265 [==============================] - 65s 755us/step - loss: 0.0256 - acc: 0.9897 - val_loss: 0.0558 - val_acc: 0.9818\n",
      "Epoch 13/1000\n",
      "86265/86265 [==============================] - 68s 786us/step - loss: 0.0233 - acc: 0.9905 - val_loss: 0.0574 - val_acc: 0.9825\n",
      "Epoch 14/1000\n",
      "86016/86265 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9913\n",
      "Epoch 00014: reducing learning rate to 0.0005000000237487257.\n",
      "86265/86265 [==============================] - 71s 825us/step - loss: 0.0223 - acc: 0.9913 - val_loss: 0.0628 - val_acc: 0.9821\n",
      "Epoch 15/1000\n",
      "86265/86265 [==============================] - 67s 772us/step - loss: 0.0182 - acc: 0.9928 - val_loss: 0.0624 - val_acc: 0.9818\n",
      "Epoch 16/1000\n",
      "86265/86265 [==============================] - 68s 788us/step - loss: 0.0161 - acc: 0.9937 - val_loss: 0.0646 - val_acc: 0.9821\n",
      "Epoch 17/1000\n",
      "86265/86265 [==============================] - 69s 803us/step - loss: 0.0152 - acc: 0.9941 - val_loss: 0.0691 - val_acc: 0.9814\n",
      "Epoch 18/1000\n",
      "86265/86265 [==============================] - 70s 817us/step - loss: 0.0138 - acc: 0.9947 - val_loss: 0.0732 - val_acc: 0.9811\n",
      "Epoch 19/1000\n",
      "86265/86265 [==============================] - 71s 820us/step - loss: 0.0134 - acc: 0.9948 - val_loss: 0.0763 - val_acc: 0.9821\n",
      "Epoch 20/1000\n",
      "86265/86265 [==============================] - 71s 821us/step - loss: 0.0122 - acc: 0.9952 - val_loss: 0.0782 - val_acc: 0.9818\n",
      "Epoch 21/1000\n",
      "86016/86265 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9956\n",
      "Epoch 00021: reducing learning rate to 0.0002500000118743628.\n",
      "86265/86265 [==============================] - 71s 828us/step - loss: 0.0114 - acc: 0.9956 - val_loss: 0.0768 - val_acc: 0.9820\n",
      "Epoch 00021: early stopping\n",
      "Predicting on validation data.\n",
      "Validation split - standard deviation for original target values: 0.18482944169037246 \n",
      "                  for predicted target values: 0.1589135229587555 \n",
      " \n",
      "\n",
      "Predicting on test data.\n",
      "(1, 9586, 6) (1, 226998, 6)\n"
     ]
    }
   ],
   "source": [
    "model_parameters = {\n",
    "    'lstm_units': 128,\n",
    "    'bidirectional': False,\n",
    "    'nb_words': nb_words,\n",
    "    'embedding_dim': embedding_dim,\n",
    "    'embedding_matrix': embedding_matrix,\n",
    "    'sequence_length': sequence_length,\n",
    "    'optimizer': optimizer,\n",
    "    'num_columns': X_train_mlp.shape[1],\n",
    "}\n",
    "\n",
    "pipeline_parameters = {\n",
    "    'model_name': getattr(keras_models, model_name),\n",
    "    'predict_test': True,\n",
    "    'number_epochs': 1000,\n",
    "    'batch_size': batch_size,\n",
    "    'seed': 1337,\n",
    "    'shuffle': True,\n",
    "    'verbose': True,\n",
    "    'run_save_name': run_name,\n",
    "    'load_keras_model': load_models,\n",
    "    'save_model': save_models,\n",
    "    'save_history': True,\n",
    "    'save_statistics': True,\n",
    "    'output_statistics': True,\n",
    "    'src_dir': os.getcwd(),\n",
    "}\n",
    "\n",
    "if kfold_run:\n",
    "    oof_train, oof_test = utils.run_parametrized_kfold([X_train, X_train_mlp], y_train, \n",
    "                                                       [X_test, X_test_mlp],\n",
    "                                                       pipeline_parameters,\n",
    "                                                       model_parameters,\n",
    "                                                       model_callbacks=model_callbacks,\n",
    "                                                       n_folds=n_folds,\n",
    "                                                       importance_training=importance,\n",
    "                                                       save_oof=save_oof)\n",
    "    print(oof_train.shape, oof_test.shape)\n",
    "else:\n",
    "    oof_valid, oof_test = utils.run_parametrized_bagging([X_tr, X_tr_mlp], y_tr,\n",
    "                                                         [X_val, X_val_mlp], y_val,\n",
    "                                                         [X_test, X_test_mlp],\n",
    "                                                         pipeline_parameters,\n",
    "                                                         model_parameters,\n",
    "                                                         model_callbacks=model_callbacks,\n",
    "                                                         n_bags=n_bags,\n",
    "                                                         user_split=True,\n",
    "                                                         split_size=split_size,\n",
    "                                                         importance_training=importance)\n",
    "    print(oof_valid.shape, oof_test.shape)\n",
    "\n",
    "\n",
    "if prepare_submission:\n",
    "    submission = utils.output_submission(oof_test.mean(axis=0), run_name, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6044863</td>\n",
       "      <td>1.941725e-06</td>\n",
       "      <td>2.192029e-05</td>\n",
       "      <td>2.730490e-04</td>\n",
       "      <td>1.631916e-07</td>\n",
       "      <td>1.523067e-03</td>\n",
       "      <td>2.635694e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6102620</td>\n",
       "      <td>1.055743e-05</td>\n",
       "      <td>8.463983e-07</td>\n",
       "      <td>1.287056e-07</td>\n",
       "      <td>1.474623e-06</td>\n",
       "      <td>4.477732e-07</td>\n",
       "      <td>1.855221e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14563293</td>\n",
       "      <td>4.001760e-05</td>\n",
       "      <td>4.941148e-07</td>\n",
       "      <td>7.140839e-06</td>\n",
       "      <td>8.602462e-07</td>\n",
       "      <td>3.586517e-07</td>\n",
       "      <td>5.510609e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21086297</td>\n",
       "      <td>1.191878e-05</td>\n",
       "      <td>7.214878e-07</td>\n",
       "      <td>1.247002e-07</td>\n",
       "      <td>1.483950e-07</td>\n",
       "      <td>4.787833e-07</td>\n",
       "      <td>3.034560e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22982444</td>\n",
       "      <td>3.288344e-03</td>\n",
       "      <td>1.337887e-05</td>\n",
       "      <td>6.323788e-05</td>\n",
       "      <td>2.322342e-06</td>\n",
       "      <td>1.427402e-05</td>\n",
       "      <td>7.602604e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24388733</td>\n",
       "      <td>4.210123e-08</td>\n",
       "      <td>6.264135e-07</td>\n",
       "      <td>1.428857e-06</td>\n",
       "      <td>8.536973e-09</td>\n",
       "      <td>5.107952e-11</td>\n",
       "      <td>5.788672e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26195914</td>\n",
       "      <td>1.967274e-06</td>\n",
       "      <td>1.217756e-08</td>\n",
       "      <td>3.428612e-10</td>\n",
       "      <td>8.709697e-10</td>\n",
       "      <td>3.539047e-10</td>\n",
       "      <td>5.938107e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31769073</td>\n",
       "      <td>1.944222e-04</td>\n",
       "      <td>4.768367e-06</td>\n",
       "      <td>8.643319e-06</td>\n",
       "      <td>3.000999e-06</td>\n",
       "      <td>1.769753e-05</td>\n",
       "      <td>2.270721e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35289443</td>\n",
       "      <td>2.393291e-05</td>\n",
       "      <td>9.425373e-06</td>\n",
       "      <td>4.662940e-06</td>\n",
       "      <td>4.042770e-06</td>\n",
       "      <td>1.218085e-05</td>\n",
       "      <td>1.577528e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>38393350</td>\n",
       "      <td>1.618364e-07</td>\n",
       "      <td>9.461331e-08</td>\n",
       "      <td>1.615171e-07</td>\n",
       "      <td>8.101845e-09</td>\n",
       "      <td>1.280530e-08</td>\n",
       "      <td>1.087200e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>51720630</td>\n",
       "      <td>1.079649e-04</td>\n",
       "      <td>7.656023e-06</td>\n",
       "      <td>5.277781e-06</td>\n",
       "      <td>3.678206e-05</td>\n",
       "      <td>1.909837e-05</td>\n",
       "      <td>1.641744e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>52808210</td>\n",
       "      <td>2.653298e-01</td>\n",
       "      <td>8.402039e-05</td>\n",
       "      <td>3.458601e-01</td>\n",
       "      <td>2.852205e-05</td>\n",
       "      <td>4.735292e-02</td>\n",
       "      <td>2.234139e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>53780387</td>\n",
       "      <td>9.401866e-04</td>\n",
       "      <td>3.277473e-05</td>\n",
       "      <td>3.054968e-05</td>\n",
       "      <td>1.375173e-05</td>\n",
       "      <td>2.645207e-05</td>\n",
       "      <td>2.139437e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>55969236</td>\n",
       "      <td>1.387735e-06</td>\n",
       "      <td>9.620628e-07</td>\n",
       "      <td>1.189039e-05</td>\n",
       "      <td>1.588913e-07</td>\n",
       "      <td>3.593550e-06</td>\n",
       "      <td>1.084005e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>59321043</td>\n",
       "      <td>7.085814e-05</td>\n",
       "      <td>3.779410e-06</td>\n",
       "      <td>4.854996e-07</td>\n",
       "      <td>5.465741e-06</td>\n",
       "      <td>1.972442e-06</td>\n",
       "      <td>8.147288e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>59993753</td>\n",
       "      <td>7.447542e-06</td>\n",
       "      <td>1.329618e-06</td>\n",
       "      <td>5.809477e-08</td>\n",
       "      <td>3.403368e-08</td>\n",
       "      <td>1.896646e-08</td>\n",
       "      <td>3.585203e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>60087415</td>\n",
       "      <td>3.327120e-10</td>\n",
       "      <td>1.809843e-07</td>\n",
       "      <td>1.730219e-08</td>\n",
       "      <td>7.706235e-07</td>\n",
       "      <td>6.060358e-07</td>\n",
       "      <td>6.380073e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>62246374</td>\n",
       "      <td>6.923631e-04</td>\n",
       "      <td>1.559416e-05</td>\n",
       "      <td>1.602054e-05</td>\n",
       "      <td>6.787978e-06</td>\n",
       "      <td>5.601449e-06</td>\n",
       "      <td>1.747311e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>63082469</td>\n",
       "      <td>1.507177e-11</td>\n",
       "      <td>2.366296e-08</td>\n",
       "      <td>1.332368e-05</td>\n",
       "      <td>1.637207e-07</td>\n",
       "      <td>8.717051e-08</td>\n",
       "      <td>1.845468e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>66675140</td>\n",
       "      <td>1.856727e-05</td>\n",
       "      <td>6.212795e-06</td>\n",
       "      <td>3.506217e-06</td>\n",
       "      <td>2.116873e-05</td>\n",
       "      <td>2.600413e-06</td>\n",
       "      <td>3.644369e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>68336055</td>\n",
       "      <td>6.850716e-05</td>\n",
       "      <td>1.592397e-06</td>\n",
       "      <td>3.617205e-06</td>\n",
       "      <td>2.578519e-06</td>\n",
       "      <td>3.343778e-06</td>\n",
       "      <td>3.358562e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>70790434</td>\n",
       "      <td>4.225989e-08</td>\n",
       "      <td>2.311109e-07</td>\n",
       "      <td>1.230699e-07</td>\n",
       "      <td>2.737504e-07</td>\n",
       "      <td>1.275645e-08</td>\n",
       "      <td>2.929447e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>71559114</td>\n",
       "      <td>7.021453e-06</td>\n",
       "      <td>1.940502e-06</td>\n",
       "      <td>5.250603e-07</td>\n",
       "      <td>2.326690e-06</td>\n",
       "      <td>1.066108e-05</td>\n",
       "      <td>2.125133e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>72407499</td>\n",
       "      <td>5.718244e-06</td>\n",
       "      <td>2.905885e-07</td>\n",
       "      <td>4.653987e-07</td>\n",
       "      <td>5.737602e-08</td>\n",
       "      <td>1.591532e-07</td>\n",
       "      <td>1.243623e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>75557079</td>\n",
       "      <td>1.093415e-05</td>\n",
       "      <td>3.357211e-06</td>\n",
       "      <td>2.064282e-06</td>\n",
       "      <td>1.581404e-06</td>\n",
       "      <td>1.784101e-06</td>\n",
       "      <td>1.567583e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>75694081</td>\n",
       "      <td>8.240009e-07</td>\n",
       "      <td>5.743789e-07</td>\n",
       "      <td>9.368258e-08</td>\n",
       "      <td>2.262634e-06</td>\n",
       "      <td>5.161328e-07</td>\n",
       "      <td>1.643049e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>82999985</td>\n",
       "      <td>8.743492e-07</td>\n",
       "      <td>2.517762e-07</td>\n",
       "      <td>4.187364e-07</td>\n",
       "      <td>6.457284e-09</td>\n",
       "      <td>5.340429e-07</td>\n",
       "      <td>2.300525e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>84135100</td>\n",
       "      <td>1.284003e-04</td>\n",
       "      <td>6.288127e-06</td>\n",
       "      <td>6.054130e-06</td>\n",
       "      <td>4.946151e-06</td>\n",
       "      <td>1.029721e-05</td>\n",
       "      <td>4.801725e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>88993345</td>\n",
       "      <td>1.548475e-05</td>\n",
       "      <td>4.285494e-06</td>\n",
       "      <td>1.166526e-06</td>\n",
       "      <td>7.419519e-06</td>\n",
       "      <td>1.354244e-06</td>\n",
       "      <td>5.285513e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>90125467</td>\n",
       "      <td>7.691443e-06</td>\n",
       "      <td>1.211064e-06</td>\n",
       "      <td>1.023174e-06</td>\n",
       "      <td>3.744823e-07</td>\n",
       "      <td>1.844347e-07</td>\n",
       "      <td>5.178709e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226968</th>\n",
       "      <td>999890906666</td>\n",
       "      <td>4.168422e-05</td>\n",
       "      <td>1.001772e-05</td>\n",
       "      <td>3.153155e-06</td>\n",
       "      <td>1.294753e-06</td>\n",
       "      <td>2.177985e-05</td>\n",
       "      <td>4.396519e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226969</th>\n",
       "      <td>999891110310</td>\n",
       "      <td>1.783786e-05</td>\n",
       "      <td>7.540744e-06</td>\n",
       "      <td>3.297442e-06</td>\n",
       "      <td>5.000938e-06</td>\n",
       "      <td>4.770159e-06</td>\n",
       "      <td>5.669980e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226970</th>\n",
       "      <td>999901299717</td>\n",
       "      <td>3.878307e-05</td>\n",
       "      <td>4.299482e-06</td>\n",
       "      <td>3.766891e-06</td>\n",
       "      <td>6.983037e-07</td>\n",
       "      <td>3.307098e-06</td>\n",
       "      <td>1.366510e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226971</th>\n",
       "      <td>999902107116</td>\n",
       "      <td>1.386830e-02</td>\n",
       "      <td>1.145817e-04</td>\n",
       "      <td>1.059050e-04</td>\n",
       "      <td>1.423807e-04</td>\n",
       "      <td>6.754276e-04</td>\n",
       "      <td>2.058327e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226972</th>\n",
       "      <td>999902754612</td>\n",
       "      <td>7.001990e-10</td>\n",
       "      <td>3.237664e-08</td>\n",
       "      <td>2.604700e-11</td>\n",
       "      <td>2.412480e-07</td>\n",
       "      <td>8.702893e-09</td>\n",
       "      <td>7.258030e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226973</th>\n",
       "      <td>999904175486</td>\n",
       "      <td>3.112741e-07</td>\n",
       "      <td>1.369620e-06</td>\n",
       "      <td>2.873195e-07</td>\n",
       "      <td>2.951697e-07</td>\n",
       "      <td>5.395718e-08</td>\n",
       "      <td>4.737111e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226974</th>\n",
       "      <td>999905240338</td>\n",
       "      <td>1.293819e-02</td>\n",
       "      <td>2.485019e-05</td>\n",
       "      <td>2.576115e-06</td>\n",
       "      <td>1.060738e-06</td>\n",
       "      <td>1.606296e-04</td>\n",
       "      <td>1.732667e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226975</th>\n",
       "      <td>999906741497</td>\n",
       "      <td>1.804015e-05</td>\n",
       "      <td>8.106870e-07</td>\n",
       "      <td>1.034240e-06</td>\n",
       "      <td>1.558455e-06</td>\n",
       "      <td>6.037168e-07</td>\n",
       "      <td>2.296265e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226976</th>\n",
       "      <td>999911717743</td>\n",
       "      <td>1.691638e-05</td>\n",
       "      <td>4.153352e-05</td>\n",
       "      <td>2.608927e-04</td>\n",
       "      <td>9.996451e-06</td>\n",
       "      <td>2.034258e-04</td>\n",
       "      <td>5.311417e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226977</th>\n",
       "      <td>999918907071</td>\n",
       "      <td>1.346010e-04</td>\n",
       "      <td>1.130406e-05</td>\n",
       "      <td>3.667023e-05</td>\n",
       "      <td>1.945097e-06</td>\n",
       "      <td>2.825185e-05</td>\n",
       "      <td>8.244228e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226978</th>\n",
       "      <td>999920895395</td>\n",
       "      <td>2.215992e-07</td>\n",
       "      <td>7.772494e-10</td>\n",
       "      <td>9.693499e-12</td>\n",
       "      <td>1.185190e-11</td>\n",
       "      <td>4.276332e-13</td>\n",
       "      <td>7.259960e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226979</th>\n",
       "      <td>999921181305</td>\n",
       "      <td>5.943349e-06</td>\n",
       "      <td>6.647716e-08</td>\n",
       "      <td>6.850103e-06</td>\n",
       "      <td>8.378092e-07</td>\n",
       "      <td>6.569259e-07</td>\n",
       "      <td>7.894207e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226980</th>\n",
       "      <td>999921616383</td>\n",
       "      <td>2.114235e-03</td>\n",
       "      <td>1.458483e-04</td>\n",
       "      <td>1.849192e-04</td>\n",
       "      <td>3.783178e-04</td>\n",
       "      <td>3.933348e-04</td>\n",
       "      <td>1.495067e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226981</th>\n",
       "      <td>999924773922</td>\n",
       "      <td>7.180035e-06</td>\n",
       "      <td>1.706431e-05</td>\n",
       "      <td>9.843594e-06</td>\n",
       "      <td>9.428942e-06</td>\n",
       "      <td>2.082243e-05</td>\n",
       "      <td>1.381791e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226982</th>\n",
       "      <td>999927268950</td>\n",
       "      <td>9.748111e-10</td>\n",
       "      <td>1.656317e-08</td>\n",
       "      <td>9.922475e-10</td>\n",
       "      <td>2.102765e-09</td>\n",
       "      <td>2.315934e-08</td>\n",
       "      <td>5.617282e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226983</th>\n",
       "      <td>999937028166</td>\n",
       "      <td>1.814134e-10</td>\n",
       "      <td>4.250878e-09</td>\n",
       "      <td>1.240235e-09</td>\n",
       "      <td>4.204687e-10</td>\n",
       "      <td>1.528727e-10</td>\n",
       "      <td>1.999621e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226984</th>\n",
       "      <td>999939655437</td>\n",
       "      <td>7.292822e-08</td>\n",
       "      <td>1.220618e-06</td>\n",
       "      <td>4.649067e-07</td>\n",
       "      <td>1.099535e-06</td>\n",
       "      <td>9.603880e-07</td>\n",
       "      <td>6.399268e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226985</th>\n",
       "      <td>999941555776</td>\n",
       "      <td>1.991796e-05</td>\n",
       "      <td>5.904301e-06</td>\n",
       "      <td>4.393887e-06</td>\n",
       "      <td>3.326666e-05</td>\n",
       "      <td>3.497317e-06</td>\n",
       "      <td>5.323291e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226986</th>\n",
       "      <td>999942678927</td>\n",
       "      <td>2.186276e-08</td>\n",
       "      <td>1.300057e-07</td>\n",
       "      <td>2.852011e-07</td>\n",
       "      <td>1.969594e-08</td>\n",
       "      <td>1.736355e-08</td>\n",
       "      <td>1.479006e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226987</th>\n",
       "      <td>999943838330</td>\n",
       "      <td>1.350261e-05</td>\n",
       "      <td>4.000436e-05</td>\n",
       "      <td>4.888594e-05</td>\n",
       "      <td>1.340964e-05</td>\n",
       "      <td>5.374012e-05</td>\n",
       "      <td>2.546451e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226988</th>\n",
       "      <td>999956516304</td>\n",
       "      <td>2.722483e-04</td>\n",
       "      <td>2.112914e-05</td>\n",
       "      <td>7.856317e-06</td>\n",
       "      <td>1.018501e-05</td>\n",
       "      <td>1.436177e-04</td>\n",
       "      <td>8.548212e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226989</th>\n",
       "      <td>999960677672</td>\n",
       "      <td>8.704752e-09</td>\n",
       "      <td>2.391593e-07</td>\n",
       "      <td>3.809102e-10</td>\n",
       "      <td>4.799285e-07</td>\n",
       "      <td>8.477782e-08</td>\n",
       "      <td>2.474538e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226990</th>\n",
       "      <td>999963214716</td>\n",
       "      <td>7.518736e-09</td>\n",
       "      <td>8.196377e-09</td>\n",
       "      <td>2.572331e-08</td>\n",
       "      <td>1.258814e-07</td>\n",
       "      <td>3.254399e-08</td>\n",
       "      <td>1.660366e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226991</th>\n",
       "      <td>999964222196</td>\n",
       "      <td>8.507655e-06</td>\n",
       "      <td>9.115040e-06</td>\n",
       "      <td>2.316224e-06</td>\n",
       "      <td>2.863052e-06</td>\n",
       "      <td>4.289374e-06</td>\n",
       "      <td>1.522395e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226992</th>\n",
       "      <td>999964949747</td>\n",
       "      <td>4.037821e-05</td>\n",
       "      <td>1.167629e-06</td>\n",
       "      <td>5.463719e-06</td>\n",
       "      <td>7.747548e-08</td>\n",
       "      <td>1.758484e-05</td>\n",
       "      <td>2.352835e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226993</th>\n",
       "      <td>999966872214</td>\n",
       "      <td>2.385855e-05</td>\n",
       "      <td>7.219219e-06</td>\n",
       "      <td>5.452653e-06</td>\n",
       "      <td>3.726778e-06</td>\n",
       "      <td>5.578797e-06</td>\n",
       "      <td>4.258487e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226994</th>\n",
       "      <td>999968525410</td>\n",
       "      <td>3.508538e-07</td>\n",
       "      <td>1.462526e-07</td>\n",
       "      <td>3.876156e-07</td>\n",
       "      <td>4.574839e-07</td>\n",
       "      <td>1.477256e-07</td>\n",
       "      <td>2.645705e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226995</th>\n",
       "      <td>999980053494</td>\n",
       "      <td>2.282806e-06</td>\n",
       "      <td>1.075564e-05</td>\n",
       "      <td>4.669308e-06</td>\n",
       "      <td>6.003472e-06</td>\n",
       "      <td>6.008317e-06</td>\n",
       "      <td>1.064197e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226996</th>\n",
       "      <td>999980680364</td>\n",
       "      <td>4.704632e-07</td>\n",
       "      <td>5.974538e-07</td>\n",
       "      <td>1.138994e-07</td>\n",
       "      <td>2.718041e-07</td>\n",
       "      <td>1.353529e-07</td>\n",
       "      <td>5.482800e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226997</th>\n",
       "      <td>999997819802</td>\n",
       "      <td>4.511061e-07</td>\n",
       "      <td>5.981556e-07</td>\n",
       "      <td>2.442552e-08</td>\n",
       "      <td>1.577762e-06</td>\n",
       "      <td>5.791433e-08</td>\n",
       "      <td>3.286732e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>226998 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id         toxic  severe_toxic       obscene        threat  \\\n",
       "0            6044863  1.941725e-06  2.192029e-05  2.730490e-04  1.631916e-07   \n",
       "1            6102620  1.055743e-05  8.463983e-07  1.287056e-07  1.474623e-06   \n",
       "2           14563293  4.001760e-05  4.941148e-07  7.140839e-06  8.602462e-07   \n",
       "3           21086297  1.191878e-05  7.214878e-07  1.247002e-07  1.483950e-07   \n",
       "4           22982444  3.288344e-03  1.337887e-05  6.323788e-05  2.322342e-06   \n",
       "5           24388733  4.210123e-08  6.264135e-07  1.428857e-06  8.536973e-09   \n",
       "6           26195914  1.967274e-06  1.217756e-08  3.428612e-10  8.709697e-10   \n",
       "7           31769073  1.944222e-04  4.768367e-06  8.643319e-06  3.000999e-06   \n",
       "8           35289443  2.393291e-05  9.425373e-06  4.662940e-06  4.042770e-06   \n",
       "9           38393350  1.618364e-07  9.461331e-08  1.615171e-07  8.101845e-09   \n",
       "10          51720630  1.079649e-04  7.656023e-06  5.277781e-06  3.678206e-05   \n",
       "11          52808210  2.653298e-01  8.402039e-05  3.458601e-01  2.852205e-05   \n",
       "12          53780387  9.401866e-04  3.277473e-05  3.054968e-05  1.375173e-05   \n",
       "13          55969236  1.387735e-06  9.620628e-07  1.189039e-05  1.588913e-07   \n",
       "14          59321043  7.085814e-05  3.779410e-06  4.854996e-07  5.465741e-06   \n",
       "15          59993753  7.447542e-06  1.329618e-06  5.809477e-08  3.403368e-08   \n",
       "16          60087415  3.327120e-10  1.809843e-07  1.730219e-08  7.706235e-07   \n",
       "17          62246374  6.923631e-04  1.559416e-05  1.602054e-05  6.787978e-06   \n",
       "18          63082469  1.507177e-11  2.366296e-08  1.332368e-05  1.637207e-07   \n",
       "19          66675140  1.856727e-05  6.212795e-06  3.506217e-06  2.116873e-05   \n",
       "20          68336055  6.850716e-05  1.592397e-06  3.617205e-06  2.578519e-06   \n",
       "21          70790434  4.225989e-08  2.311109e-07  1.230699e-07  2.737504e-07   \n",
       "22          71559114  7.021453e-06  1.940502e-06  5.250603e-07  2.326690e-06   \n",
       "23          72407499  5.718244e-06  2.905885e-07  4.653987e-07  5.737602e-08   \n",
       "24          75557079  1.093415e-05  3.357211e-06  2.064282e-06  1.581404e-06   \n",
       "25          75694081  8.240009e-07  5.743789e-07  9.368258e-08  2.262634e-06   \n",
       "26          82999985  8.743492e-07  2.517762e-07  4.187364e-07  6.457284e-09   \n",
       "27          84135100  1.284003e-04  6.288127e-06  6.054130e-06  4.946151e-06   \n",
       "28          88993345  1.548475e-05  4.285494e-06  1.166526e-06  7.419519e-06   \n",
       "29          90125467  7.691443e-06  1.211064e-06  1.023174e-06  3.744823e-07   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "226968  999890906666  4.168422e-05  1.001772e-05  3.153155e-06  1.294753e-06   \n",
       "226969  999891110310  1.783786e-05  7.540744e-06  3.297442e-06  5.000938e-06   \n",
       "226970  999901299717  3.878307e-05  4.299482e-06  3.766891e-06  6.983037e-07   \n",
       "226971  999902107116  1.386830e-02  1.145817e-04  1.059050e-04  1.423807e-04   \n",
       "226972  999902754612  7.001990e-10  3.237664e-08  2.604700e-11  2.412480e-07   \n",
       "226973  999904175486  3.112741e-07  1.369620e-06  2.873195e-07  2.951697e-07   \n",
       "226974  999905240338  1.293819e-02  2.485019e-05  2.576115e-06  1.060738e-06   \n",
       "226975  999906741497  1.804015e-05  8.106870e-07  1.034240e-06  1.558455e-06   \n",
       "226976  999911717743  1.691638e-05  4.153352e-05  2.608927e-04  9.996451e-06   \n",
       "226977  999918907071  1.346010e-04  1.130406e-05  3.667023e-05  1.945097e-06   \n",
       "226978  999920895395  2.215992e-07  7.772494e-10  9.693499e-12  1.185190e-11   \n",
       "226979  999921181305  5.943349e-06  6.647716e-08  6.850103e-06  8.378092e-07   \n",
       "226980  999921616383  2.114235e-03  1.458483e-04  1.849192e-04  3.783178e-04   \n",
       "226981  999924773922  7.180035e-06  1.706431e-05  9.843594e-06  9.428942e-06   \n",
       "226982  999927268950  9.748111e-10  1.656317e-08  9.922475e-10  2.102765e-09   \n",
       "226983  999937028166  1.814134e-10  4.250878e-09  1.240235e-09  4.204687e-10   \n",
       "226984  999939655437  7.292822e-08  1.220618e-06  4.649067e-07  1.099535e-06   \n",
       "226985  999941555776  1.991796e-05  5.904301e-06  4.393887e-06  3.326666e-05   \n",
       "226986  999942678927  2.186276e-08  1.300057e-07  2.852011e-07  1.969594e-08   \n",
       "226987  999943838330  1.350261e-05  4.000436e-05  4.888594e-05  1.340964e-05   \n",
       "226988  999956516304  2.722483e-04  2.112914e-05  7.856317e-06  1.018501e-05   \n",
       "226989  999960677672  8.704752e-09  2.391593e-07  3.809102e-10  4.799285e-07   \n",
       "226990  999963214716  7.518736e-09  8.196377e-09  2.572331e-08  1.258814e-07   \n",
       "226991  999964222196  8.507655e-06  9.115040e-06  2.316224e-06  2.863052e-06   \n",
       "226992  999964949747  4.037821e-05  1.167629e-06  5.463719e-06  7.747548e-08   \n",
       "226993  999966872214  2.385855e-05  7.219219e-06  5.452653e-06  3.726778e-06   \n",
       "226994  999968525410  3.508538e-07  1.462526e-07  3.876156e-07  4.574839e-07   \n",
       "226995  999980053494  2.282806e-06  1.075564e-05  4.669308e-06  6.003472e-06   \n",
       "226996  999980680364  4.704632e-07  5.974538e-07  1.138994e-07  2.718041e-07   \n",
       "226997  999997819802  4.511061e-07  5.981556e-07  2.442552e-08  1.577762e-06   \n",
       "\n",
       "              insult  identity_hate  \n",
       "0       1.523067e-03   2.635694e-05  \n",
       "1       4.477732e-07   1.855221e-05  \n",
       "2       3.586517e-07   5.510609e-07  \n",
       "3       4.787833e-07   3.034560e-07  \n",
       "4       1.427402e-05   7.602604e-06  \n",
       "5       5.107952e-11   5.788672e-12  \n",
       "6       3.539047e-10   5.938107e-09  \n",
       "7       1.769753e-05   2.270721e-06  \n",
       "8       1.218085e-05   1.577528e-05  \n",
       "9       1.280530e-08   1.087200e-09  \n",
       "10      1.909837e-05   1.641744e-05  \n",
       "11      4.735292e-02   2.234139e-05  \n",
       "12      2.645207e-05   2.139437e-05  \n",
       "13      3.593550e-06   1.084005e-06  \n",
       "14      1.972442e-06   8.147288e-06  \n",
       "15      1.896646e-08   3.585203e-08  \n",
       "16      6.060358e-07   6.380073e-07  \n",
       "17      5.601449e-06   1.747311e-05  \n",
       "18      8.717051e-08   1.845468e-09  \n",
       "19      2.600413e-06   3.644369e-06  \n",
       "20      3.343778e-06   3.358562e-06  \n",
       "21      1.275645e-08   2.929447e-07  \n",
       "22      1.066108e-05   2.125133e-06  \n",
       "23      1.591532e-07   1.243623e-08  \n",
       "24      1.784101e-06   1.567583e-06  \n",
       "25      5.161328e-07   1.643049e-06  \n",
       "26      5.340429e-07   2.300525e-08  \n",
       "27      1.029721e-05   4.801725e-06  \n",
       "28      1.354244e-06   5.285513e-06  \n",
       "29      1.844347e-07   5.178709e-07  \n",
       "...              ...            ...  \n",
       "226968  2.177985e-05   4.396519e-06  \n",
       "226969  4.770159e-06   5.669980e-06  \n",
       "226970  3.307098e-06   1.366510e-07  \n",
       "226971  6.754276e-04   2.058327e-04  \n",
       "226972  8.702893e-09   7.258030e-05  \n",
       "226973  5.395718e-08   4.737111e-07  \n",
       "226974  1.606296e-04   1.732667e-04  \n",
       "226975  6.037168e-07   2.296265e-06  \n",
       "226976  2.034258e-04   5.311417e-05  \n",
       "226977  2.825185e-05   8.244228e-06  \n",
       "226978  4.276332e-13   7.259960e-11  \n",
       "226979  6.569259e-07   7.894207e-07  \n",
       "226980  3.933348e-04   1.495067e-04  \n",
       "226981  2.082243e-05   1.381791e-05  \n",
       "226982  2.315934e-08   5.617282e-10  \n",
       "226983  1.528727e-10   1.999621e-10  \n",
       "226984  9.603880e-07   6.399268e-07  \n",
       "226985  3.497317e-06   5.323291e-06  \n",
       "226986  1.736355e-08   1.479006e-08  \n",
       "226987  5.374012e-05   2.546451e-05  \n",
       "226988  1.436177e-04   8.548212e-06  \n",
       "226989  8.477782e-08   2.474538e-06  \n",
       "226990  3.254399e-08   1.660366e-07  \n",
       "226991  4.289374e-06   1.522395e-05  \n",
       "226992  1.758484e-05   2.352835e-08  \n",
       "226993  5.578797e-06   4.258487e-06  \n",
       "226994  1.477256e-07   2.645705e-07  \n",
       "226995  6.008317e-06   1.064197e-05  \n",
       "226996  1.353529e-07   5.482800e-07  \n",
       "226997  5.791433e-08   3.286732e-07  \n",
       "\n",
       "[226998 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
