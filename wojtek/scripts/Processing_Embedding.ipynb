{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/CVxTz/keras-bidirectional-lstm-baseline-lb-0-051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/w/anaconda3/envs/idp3exp/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/w/anaconda3/envs/idp3exp/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText2_BasicClean2_300dim_200k_320len_random0\n",
      "Load data with basic cleaning with non-alphanumeric contained.\n"
     ]
    }
   ],
   "source": [
    "embedding_type = 'FastText2'\n",
    "data_type = 'BasicClean2'\n",
    "embedding_source = 'crawl-300d-2M.vec'\n",
    "\n",
    "max_features = 200000\n",
    "max_features_k = int(max_features / 1e3)\n",
    "sequence_length = 320\n",
    "embedding_dim = 300\n",
    "create_embedding = True\n",
    "random_init = False\n",
    "\n",
    "\n",
    "emb_mean, emb_std = 0.020940498, 0.6441043\n",
    "embedding_filename = '{}_{}_{}dim_{}k_{}len_random{}'.format(\n",
    "    embedding_type, data_type, embedding_dim, max_features_k, sequence_length, int(random_init))\n",
    "print(embedding_filename)\n",
    "\n",
    "\n",
    "train, test = utils.load_data('../data/', data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of words: 200001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(train.comment_text.tolist() + test.comment_text.tolist())\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index)) + 1\n",
    "print('Num of words: {}'.format(nb_words))\n",
    "\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=sequence_length)\n",
    "y_train = train[list_classes].values\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=sequence_length)\n",
    "\n",
    "del train, test, list_tokenized_train, list_tokenized_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "X_train.columns = ['word{}'.format(x) for x in range(X_train.shape[1])]\n",
    "\n",
    "X_test = pd.DataFrame(X_test)\n",
    "X_test.columns = ['word{}'.format(x) for x in range(X_test.shape[1])]\n",
    "\n",
    "tokenized = pd.concat([X_train, X_test])\n",
    "tokenized.to_pickle('../data/features/data_TokenizedSentences160.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText & GloVe.gensim"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if create_embedding:\n",
    "    embedding_file = '/home/w/Projects/Toxic/data/embeddings/{}'.format(embedding_source)\n",
    "    word2vec = KeyedVectors.load_word2vec_format(embedding_file, binary=False)\n",
    "    print('Found %s word vectors of word2vec' % len(word2vec.vocab))\n",
    "    if random_init:\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_dim))\n",
    "    else:\n",
    "        embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if word in word2vec.vocab:\n",
    "            embedding_matrix[i] = word2vec.word_vec(word)\n",
    "    print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "    pd.to_pickle(embedding_matrix, '../data/embeddings/{}.pkl'.format(embedding_filename))\n",
    "else:\n",
    "    embedding_matrix = pd.read_pickle('../data/embeddings/{}.pkl'.format(embedding_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 2000000 word vectors.\n",
      "Null word embeddings: 92441\n"
     ]
    }
   ],
   "source": [
    "if create_embedding:\n",
    "    embedding_file = '/home/w/Projects/Toxic/data/embeddings/{}'.format(embedding_source)\n",
    "    embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(embedding_file))\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "    if random_init:\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_dim))\n",
    "    else:\n",
    "        embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "    pd.to_pickle(embedding_matrix, '../data/embeddings/{}.pkl'.format(embedding_filename))\n",
    "else:\n",
    "    embedding_matrix = pd.read_pickle('../data/embeddings/{}.pkl'.format(embedding_filename))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "words = word2vec.index2word\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "embedding_matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
